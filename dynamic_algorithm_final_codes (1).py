# -*- coding: utf-8 -*-
"""Dynamic algorithm final codes

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11aN0WPeAGASchRaOjZ541NSigF1Rkhvu
"""

!pip install scikit-multiflow

"""**HOEFFDING TREE CLASSIFIER**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from skmultiflow.trees import HoeffdingTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def data_stream(X, y, batch_size=1000):
    for i in range(0, len(X), batch_size):
        yield X[i:i+batch_size], y[i:i+batch_size]
data = pd.read_csv('UROPDATASET2.csv')

threshold = data['Intensity59'].median()
data['Label'] = data['Intensity59'].apply(lambda x: 1 if x > threshold else 0)

X = data.drop(columns=['Id', 'ActivityHour', 'Intensity59', 'Label']).values
y = data['Label'].values
split_index = int(0.7 * len(data))
X_train, X_test = X[:split_index], X[split_index:]
y_train, y_test = y[:split_index], y[split_index:]
ht = HoeffdingTreeClassifier()
accuracy_list = []
precision_list = []
recall_list = []
f1_list = []
batch_size = 1000
for stream_batch_X, stream_batch_y in data_stream(X_train, y_train, batch_size=batch_size):
    ht = ht.partial_fit(stream_batch_X, stream_batch_y, classes=np.unique(y))
    y_pred = ht.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    accuracy_list.append(accuracy)
    precision_list.append(precision)
    recall_list.append(recall)
    f1_list.append(f1)
    print(f"Iteration {len(accuracy_list)} - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}")
plt.figure(figsize=(10, 6))

plt.plot(accuracy_list, label='Accuracy')
plt.plot(precision_list, label='Precision')
plt.plot(recall_list, label='Recall')
plt.plot(f1_list, label='F1-Score')

plt.title('Model Performance Over Streaming Data')
plt.xlabel('Streaming Iteration')
plt.ylabel('Metric Value')
plt.legend()
plt.show()

"""**SGD CLASSIFIER**"""

import pandas as pd
import numpy as np
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

def data_stream(X, y, batch_size=1000):
    for i in range(0, len(X), batch_size):
        yield X.iloc[i:i+batch_size], y.iloc[i:i+batch_size]
data = pd.read_csv('UROPDATASET2.csv')

data['ActivityHour'] = pd.to_datetime(data['ActivityHour'], errors='coerce')
data['ActivityHour'] = data['ActivityHour'].dt.strftime('%d/%m/%Y')
data = data.dropna(subset=['ActivityHour'])
X = data.loc[:, 'Intensity01':'Intensity59']
y = data['Intensity00']
sgd = SGDClassifier()
scaler = StandardScaler()
unique_classes = set()
X_scaled = scaler.fit_transform(X)
sgd.partial_fit(X_scaled, y, classes=np.unique(y))
unique_classes.update(np.unique(y))
accuracy_list = []
precision_list = []
recall_list = []
f1_list = []
stream_batch_size = 1000
for stream_batch_X, stream_batch_y in data_stream(X, y, batch_size=stream_batch_size):
    X_batch_scaled = scaler.transform(stream_batch_X.loc[:, 'Intensity01':'Intensity59'])
    y_batch = stream_batch_y
    unique_classes.update(np.setdiff1d(np.unique(y_batch), unique_classes))
    sgd.partial_fit(X_batch_scaled, y_batch, classes=list(unique_classes))
    predictions = sgd.predict(X_batch_scaled)
    accuracy = accuracy_score(y_batch, predictions)
    precision = precision_score(y_batch, predictions, average='weighted')
    recall = recall_score(y_batch, predictions, average='weighted')
    f1 = f1_score(y_batch, predictions, average='weighted')
    accuracy_list.append(accuracy)
    precision_list.append(precision)
    recall_list.append(recall)
    f1_list.append(f1)
    print(f"Iteration {len(accuracy_list)} - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}")
plt.figure(figsize=(10, 6))

plt.plot(accuracy_list, label='Accuracy')
plt.plot(precision_list, label='Precision')
plt.plot(recall_list, label='Recall')
plt.plot(f1_list, label='F1-Score')

plt.title('Model Performance Over Streaming Data')
plt.xlabel('Streaming Iteration')
plt.ylabel('Metric Value')
plt.legend()
plt.show()

"""**STREAMING K MEANS **"""

import pandas as pd
import numpy as np
from sklearn.cluster import MiniBatchKMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

def data_stream(X, batch_size=1000):
    for i in range(0, len(X), batch_size):
        yield X.iloc[i:i + batch_size]

data = pd.read_csv('UROPDATASET2.csv')
data['ActivityHour'] = pd.to_datetime(data['ActivityHour'], errors='coerce')
data['ActivityHour'] = data['ActivityHour'].dt.strftime('%d/%m/%Y')
data = data.dropna(subset=['ActivityHour'])
X = data.loc[:, 'Intensity01':'Intensity59']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

kmeans = MiniBatchKMeans(n_clusters=3, random_state=42)
silhouette_scores = []

stream_batch_size = 1000
for stream_batch_X in data_stream(X, batch_size=stream_batch_size):
    X_batch_scaled = scaler.transform(stream_batch_X.loc[:, 'Intensity01':'Intensity59'])
    kmeans.partial_fit(X_batch_scaled)

    # Evaluate clustering performance
    labels = kmeans.predict(X_batch_scaled)
    silhouette = silhouette_score(X_batch_scaled, labels)
    silhouette_scores.append(silhouette)

    print(f"Iteration {len(silhouette_scores)} - Silhouette Score: {silhouette}")

plt.figure(figsize=(10, 6))
plt.plot(silhouette_scores, label='Silhouette Score')
plt.title('Streaming K-Means Performance Over Streaming Data')
plt.xlabel('Streaming Iteration')
plt.ylabel('Silhouette Score')
plt.legend()
plt.show()

"""**PREPROCESSING **"""

import pandas as pd

original_data = pd.read_csv('UROPDATASET2.csv')
original_data['ActivityHour'] = pd.to_datetime(original_data['ActivityHour'], errors='coerce')
original_data['ActivityHour'] = original_data['ActivityHour'].dt.strftime('%d/%m/%Y')
original_data = original_data.dropna(subset=['ActivityHour'])
original_data.to_csv('UROPDATASET2_preprocessed.csv', index=False)